{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Histopathologic Cancer Detection: Results and Analysis\n",
    "\n",
    "In this notebook, we'll present and analyze the results of our models for the Histopathologic Cancer Detection task. We'll evaluate their performance, visualize predictions, analyze error patterns, and compare different approaches."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Data Loading\n",
    "\n",
    "First, let's import the necessary libraries and load the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import random\n",
    "from tqdm.notebook import tqdm\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import itertools\n",
    "\n",
    "# TensorFlow and Keras\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# Scikit-learn\n",
    "from sklearn.metrics import roc_auc_score, roc_curve, confusion_matrix, classification_report, precision_recall_curve, average_precision_score\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# Set plot style\n",
    "plt.style.use('seaborn-whitegrid')\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['font.size'] = 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set paths to the dataset and models\n",
    "BASE_DIR = '../data'\n",
    "TRAIN_DIR = os.path.join(BASE_DIR, 'train')\n",
    "TEST_DIR = os.path.join(BASE_DIR, 'test')\n",
    "TRAIN_LABELS_PATH = os.path.join(BASE_DIR, 'train_labels.csv')\n",
    "MODELS_DIR = '../models'\n",
    "\n",
    "# Create models directory if it doesn't exist\n",
    "os.makedirs(MODELS_DIR, exist_ok=True)\n",
    "\n",
    "# Load the training labels\n",
    "try:\n",
    "    train_labels = pd.read_csv(TRAIN_LABELS_PATH)\n",
    "    print(f\"Successfully loaded training labels with shape: {train_labels.shape}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading training labels: {e}\")\n",
    "    print(\"Please ensure the dataset is downloaded and the paths are correctly set.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Trained Models\n",
    "\n",
    "Let's load the models we trained in the previous notebook. If the models haven't been trained yet, you'll need to run the training code in the Model Architecture notebook first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_trained_model(model_path):\n",
    "    \"\"\"Load a trained model from the specified path\"\"\"\n",
    "    try:\n",
    "        model = load_model(model_path)\n",
    "        print(f\"Successfully loaded model from {model_path}\")\n",
    "        return model\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading model from {model_path}: {e}\")\n",
    "        print(\"Please ensure the model has been trained and saved correctly.\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the trained models\n",
    "model_paths = {\n",
    "    'custom_cnn': os.path.join(MODELS_DIR, 'custom_cnn.h5'),\n",
    "    'resnet50': os.path.join(MODELS_DIR, 'resnet50.h5'),\n",
    "    'efficientnet': os.path.join(MODELS_DIR, 'efficientnet.h5'),\n",
    "    'mobilenet': os.path.join(MODELS_DIR, 'mobilenet.h5')\n",
    "}\n",
    "\n",
    "models = {}\n",
    "for name, path in model_paths.items():\n",
    "    models[name] = load_trained_model(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Prepare Validation Data\n",
    "\n",
    "Let's prepare the validation data to evaluate our models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and validation sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_df, val_df = train_test_split(\n",
    "    train_labels, \n",
    "    test_size=0.2,  # 20% for validation\n",
    "    random_state=42,\n",
    "    stratify=train_labels['label']  # Ensure class balance in both sets\n",
    ")\n",
    "\n",
    "print(f\"Training set size: {len(train_df)}\")\n",
    "print(f\"Validation set size: {len(val_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a validation data generator\n",
    "val_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "val_generator = val_datagen.flow_from_dataframe(\n",
    "    dataframe=val_df,\n",
    "    directory=TRAIN_DIR,\n",
    "    x_col='id',\n",
    "    y_col='label',\n",
    "    target_size=(96, 96),\n",
    "    batch_size=32,\n",
    "    class_mode='binary',\n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model Evaluation\n",
    "\n",
    "Now, let's evaluate our models on the validation set and compare their performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, generator):\n",
    "    \"\"\"Evaluate a model on the given data generator\"\"\"\n",
    "    if model is None:\n",
    "        return None, None, None\n",
    "    \n",
    "    # Reset the generator to the beginning\n",
    "    generator.reset()\n",
    "    \n",
    "    # Get the true labels\n",
    "    y_true = generator.classes\n",
    "    \n",
    "    # Get predictions\n",
    "    y_pred_proba = model.predict(generator, verbose=1)\n",
    "    y_pred = (y_pred_proba > 0.5).astype(int).flatten()\n",
    "    \n",
    "    # Calculate metrics\n",
    "    auc = roc_auc_score(y_true, y_pred_proba)\n",
    "    \n",
    "    return y_true, y_pred_proba, y_pred, auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate each model\n",
    "results = {}\n",
    "for name, model in models.items():\n",
    "    if model is not None:\n",
    "        print(f\"\\nEvaluating {name} model...\")\n",
    "        y_true, y_pred_proba, y_pred, auc = evaluate_model(model, val_generator)\n",
    "        results[name] = {\n",
    "            'y_true': y_true,\n",
    "            'y_pred_proba': y_pred_proba,\n",
    "            'y_pred': y_pred,\n",
    "            'auc': auc\n",
    "        }\n",
    "        print(f\"{name} AUC: {auc:.4f}\")\n",
    "        print(\"\\nClassification Report:\")\n",
    "        print(classification_report(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Confusion Matrices\n",
    "\n",
    "Let's visualize the confusion matrices for each model to better understand their performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(y_true, y_pred, classes, title, normalize=False, cmap=plt.cm.Blues):\n",
    "    \"\"\"Plot confusion matrix\"\"\"\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "    \n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title, fontsize=16)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, fontsize=12)\n",
    "    plt.yticks(tick_marks, classes, fontsize=12)\n",
    "    \n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\",\n",
    "                 fontsize=14)\n",
    "    \n",
    "    plt.ylabel('True Label', fontsize=14)\n",
    "    plt.xlabel('Predicted Label', fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot confusion matrices for each model\n",
    "classes = ['Normal', 'Cancer']\n",
    "for name, result in results.items():\n",
    "    print(f\"\\nConfusion Matrix for {name} model:\")\n",
    "    plot_confusion_matrix(result['y_true'], result['y_pred'], classes, f\"{name} Confusion Matrix\")\n",
    "    \n",
    "    print(f\"\\nNormalized Confusion Matrix for {name} model:\")\n",
    "    plot_confusion_matrix(result['y_true'], result['y_pred'], classes, f\"{name} Normalized Confusion Matrix\", normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 ROC Curves\n",
    "\n",
    "Let's plot the ROC curves for each model to visualize their performance across different classification thresholds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_roc_curves(results):\n",
    "    \"\"\"Plot ROC curves for multiple models\"\"\"\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    \n",
    "    for name, result in results.items():\n",
    "        fpr, tpr, _ = roc_curve(result['y_true'], result['y_pred_proba'])\n",
    "        auc = result['auc']\n",
    "        plt.plot(fpr, tpr, lw=2, label=f'{name} (AUC = {auc:.4f})')\n",
    "    \n",
    "    plt.plot([0, 1], [0, 1], 'k--', lw=2)\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate', fontsize=14)\n",
    "    plt.ylabel('True Positive Rate', fontsize=14)\n",
    "    plt.title('Receiver Operating Characteristic (ROC) Curves', fontsize=16)\n",
    "    plt.legend(loc=\"lower right\", fontsize=12)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot ROC curves\n",
    "plot_roc_curves(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Precision-Recall Curves\n",
    "\n",
    "Let's also plot precision-recall curves, which are particularly useful for imbalanced datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_precision_recall_curves(results):\n",
    "    \"\"\"Plot precision-recall curves for multiple models\"\"\"\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    \n",
    "    for name, result in results.items():\n",
    "        precision, recall, _ = precision_recall_curve(result['y_true'], result['y_pred_proba'])\n",
    "        ap = average_precision_score(result['y_true'], result['y_pred_proba'])\n",
    "        plt.plot(recall, precision, lw=2, label=f'{name} (AP = {ap:.4f})')\n",
    "    \n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('Recall', fontsize=14)\n",
    "    plt.ylabel('Precision', fontsize=14)\n",
    "    plt.title('Precision-Recall Curves', fontsize=16)\n",
    "    plt.legend(loc=\"lower left\", fontsize=12)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot precision-recall curves\n",
    "plot_precision_recall_curves(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Model Comparison\n",
    "\n",
    "Let's compare the performance of our models across different metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics(y_true, y_pred, y_pred_proba):\n",
    "    \"\"\"Calculate various performance metrics\"\"\"\n",
    "    from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "    \n",
    "    metrics = {\n",
    "        'accuracy': accuracy_score(y_true, y_pred),\n",
    "        'precision': precision_score(y_true, y_pred),\n",
    "        'recall': recall_score(y_true, y_pred),\n",
    "        'f1': f1_score(y_true, y_pred),\n",
    "        'auc': roc_auc_score(y_true, y_pred_proba),\n",
    "        'ap': average_precision_score(y_true, y_pred_proba)\n",
    "    }\n",
    "    \n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate metrics for each model\n",
    "metrics_dict = {}\n",
    "for name, result in results.items():\n",
    "    metrics_dict[name] = calculate_metrics(result['y_true'], result['y_pred'], result['y_pred_proba'])\n",
    "\n",
    "# Create a DataFrame for easy comparison\n",
    "metrics_df = pd.DataFrame(metrics_dict).T\n",
    "metrics_df = metrics_df.round(4)\n",
    "\n",
    "# Display the metrics\n",
    "print(\"Model Performance Metrics:\")\n",
    "display(metrics_df)\n",
    "\n",
    "# Highlight the best model for each metric\n",
    "best_model = metrics_df.idxmax()\n",
    "print(\"\\nBest model for each metric:\")\n",
    "for metric, model in best_model.items():\n",
    "    print(f\"{metric}: {model} ({metrics_df.loc[model, metric]:.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the metrics comparison\n",
    "plt.figure(figsize=(14, 8))\n",
    "metrics_df.plot(kind='bar', figsize=(14, 8))\n",
    "plt.title('Model Performance Comparison', fontsize=16)\n",
    "plt.xlabel('Model', fontsize=14)\n",
    "plt.ylabel('Score', fontsize=14)\n",
    "plt.xticks(rotation=0, fontsize=12)\n",
    "plt.legend(title='Metric', fontsize=12)\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Error Analysis\n",
    "\n",
    "Let's analyze the errors made by our best-performing model to gain insights into its limitations and potential areas for improvement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify the best model based on AUC\n",
    "best_model_name = metrics_df['auc'].idxmax()\n",
    "print(f\"Best model based on AUC: {best_model_name} (AUC = {metrics_df.loc[best_model_name, 'auc']:.4f})\")\n",
    "\n",
    "# Get the results for the best model\n",
    "best_result = results[best_model_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_preprocess_image(image_id, directory, target_size=(96, 96), normalize=True):\n",
    "    \"\"\"Load and preprocess an image from the specified directory\"\"\"\n",
    "    try:\n",
    "        img_path = os.path.join(directory, f\"{image_id}.tif\")\n",
    "        img = cv2.imread(img_path)\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)  # Convert BGR to RGB\n",
    "        \n",
    "        # Resize if needed\n",
    "        if img.shape[:2] != target_size:\n",
    "            img = cv2.resize(img, target_size)\n",
    "        \n",
    "        # Normalize pixel values to [0, 1]\n",
    "        if normalize:\n",
    "            img = img.astype(np.float32) / 255.0\n",
    "        \n",
    "        return img\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading image {image_id}: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find misclassified examples\n",
    "val_df_reset = val_df.reset_index(drop=True)\n",
    "val_df_reset['predicted'] = best_result['y_pred']\n",
    "val_df_reset['probability'] = best_result['y_pred_proba'].flatten()\n",
    "val_df_reset['correct'] = val_df_reset['label'] == val_df_reset['predicted']\n",
    "\n",
    "# False positives (predicted cancer, actually normal)\n",
    "false_positives = val_df_reset[(val_df_reset['label'] == 0) & (val_df_reset['predicted'] == 1)]\n",
    "print(f\"Number of false positives: {len(false_positives)}\")\n",
    "\n",
    "# False negatives (predicted normal, actually cancer)\n",
    "false_negatives = val_df_reset[(val_df_reset['label'] == 1) & (val_df_reset['predicted'] == 0)]\n",
    "print(f\"Number of false negatives: {len(false_negatives)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_misclassified_examples(df, category, num_examples=5):\n",
    "    \"\"\"Display misclassified examples with their predicted probabilities\"\"\"\n",
    "    if len(df) == 0:\n",
    "        print(f\"No {category} examples found.\")\n",
    "        return\n",
    "    \n",
    "    # Sort by prediction confidence (probability closest to 0.5 is least confident)\n",
    "    if category == 'false_positives':\n",
    "        df_sorted = df.sort_values(by='probability', ascending=False).head(num_examples)\n",
    "    else:  # false_negatives\n",
    "        df_sorted = df.sort_values(by='probability', ascending=True).head(num_examples)\n",
    "    \n",
    "    # Set up the figure\n",
    "    fig, axes = plt.subplots(1, num_examples, figsize=(num_examples*4, 4))\n",
    "    if num_examples == 1:\n",
    "        axes = [axes]\n",
    "    \n",
    "    # Display each example\n",
    "    for i, (_, row) in enumerate(df_sorted.iterrows()):\n",
    "        if i >= num_examples:\n",
    "            break\n",
    "            \n",
    "        img = load_and_preprocess_image(row['id'], TRAIN_DIR, normalize=False)\n",
    "        if img is not None:\n",
    "            axes[i].imshow(img)\n",
    "            true_label = 'Cancer' if row['label'] == 1 else 'Normal'\n",
    "            pred_label = 'Cancer' if row['predicted'] == 1 else 'Normal'\n",
    "            axes[i].set_title(f\"True: {true_label}\\nPred: {pred_label}\\nProb: {row['probability']:.4f}\")\n",
    "            axes[i].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.suptitle(f\"{category.replace('_', ' ').title()}\", fontsize=16, y=1.05)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display false positives\n",
    "print(\"False Positives (Normal tissue classified as Cancer):\")\n",
    "display_misclassified_examples(false_positives, 'false_positives')\n",
    "\n",
    "# Display false negatives\n",
    "print(\"\\nFalse Negatives (Cancer tissue classified as Normal):\")\n",
    "display_misclassified_examples(false_negatives, 'false_negatives')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 Analysis of Prediction Confidence\n",
    "\n",
    "Let's analyze the prediction confidence (probability) distribution for correct and incorrect predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze prediction confidence\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.histplot(data=val_df_reset, x='probability', hue='correct', bins=20, kde=True)\n",
    "plt.title('Prediction Confidence Distribution', fontsize=16)\n",
    "plt.xlabel('Predicted Probability of Cancer', fontsize=14)\n",
    "plt.ylabel('Count', fontsize=14)\n",
    "plt.legend(title='Correct Prediction', labels=['Incorrect', 'Correct'])\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze prediction confidence by true label\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.histplot(data=val_df_reset, x='probability', hue='label', bins=20, kde=True, palette=['green', 'red'])\n",
    "plt.title('Prediction Confidence by True Label', fontsize=16)\n",
    "plt.xlabel('Predicted Probability of Cancer', fontsize=14)\n",
    "plt.ylabel('Count', fontsize=14)\n",
    "plt.legend(title='True Label', labels=['Normal', 'Cancer'])\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Visualizing Model Predictions\n",
    "\n",
    "Let's visualize some examples of correct predictions to better understand what patterns the model is learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find correctly classified examples\n",
    "correct_normal = val_df_reset[(val_df_reset['label'] == 0) & (val_df_reset['correct'])]\n",
    "correct_cancer = val_df_reset[(val_df_reset['label'] == 1) & (val_df_reset['correct'])]\n",
    "\n",
    "# Get high-confidence examples\n",
    "high_conf_normal = correct_normal.sort_values(by='probability', ascending=True).head(5)\n",
    "high_conf_cancer = correct_cancer.sort_values(by='probability', ascending=False).head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display high-confidence correct examples\n",
    "print(\"High-Confidence Normal Tissue Examples (Correctly Classified):\")\n",
    "display_misclassified_examples(high_conf_normal, 'high_conf_normal', num_examples=5)\n",
    "\n",
    "print(\"\\nHigh-Confidence Cancer Tissue Examples (Correctly Classified):\")\n",
    "display_misclassified_examples(high_conf_cancer, 'high_conf_cancer', num_examples=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Threshold Optimization\n",
    "\n",
    "The default classification threshold is 0.5, but we can optimize this threshold based on our specific requirements (e.g., prioritizing sensitivity over specificity or vice versa)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_optimal_threshold(y_true, y_pred_proba, metric='f1'):\n",
    "    \"\"\"Find the optimal threshold that maximizes the specified metric\"\"\"\n",
    "    from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score\n",
    "    \n",
    "    # Define the metric function based on the specified metric\n",
    "    if metric == 'f1':\n",
    "        metric_func = lambda y_true, y_pred: f1_score(y_true, y_pred)\n",
    "    elif metric == 'precision':\n",
    "        metric_func = lambda y_true, y_pred: precision_score(y_true, y_pred)\n",
    "    elif metric == 'recall':\n",
    "        metric_func = lambda y_true, y_pred: recall_score(y_true, y_pred)\n",
    "    elif metric == 'accuracy':\n",
    "        metric_func = lambda y_true, y_pred: accuracy_score(y_true, y_pred)\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported metric: {metric}\")\n",
    "    \n",
    "    # Try different thresholds and calculate the metric\n",
    "    thresholds = np.arange(0.1, 1.0, 0.05)\n",
    "    scores = []\n",
    "    \n",
    "    for threshold in thresholds:\n",
    "        y_pred = (y_pred_proba >= threshold).astype(int).flatten()\n",
    "        score = metric_func(y_true, y_pred)\n",
    "        scores.append(score)\n",
    "    \n",
    "    # Find the threshold that maximizes the metric\n",
    "    best_score_idx = np.argmax(scores)\n",
    "    best_threshold = thresholds[best_score_idx]\n",
    "    best_score = scores[best_score_idx]\n",
    "    \n",
    "    return best_threshold, best_score, thresholds, scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find optimal thresholds for different metrics\n",
    "metrics = ['accuracy', 'precision', 'recall', 'f1']\n",
    "best_thresholds = {}\n",
    "\n",
    "for metric in metrics:\n",
    "    best_threshold, best_score, thresholds, scores = find_optimal_threshold(\n",
    "        best_result['y_true'], best_result['y_pred_proba'], metric=metric\n",
    "    )\n",
    "    best_thresholds[metric] = best_threshold\n",
    "    \n",
    "    # Plot the metric vs. threshold\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(thresholds, scores, 'o-')\n",
    "    plt.axvline(x=best_threshold, color='r', linestyle='--')\n",
    "    plt.text(best_threshold + 0.02, best_score - 0.05, \n",
    "             f'Threshold: {best_threshold:.2f}\\n{metric.capitalize()}: {best_score:.4f}',\n",
    "             fontsize=12)\n",
    "    plt.title(f'{metric.capitalize()} vs. Threshold', fontsize=16)\n",
    "    plt.xlabel('Threshold', fontsize=14)\n",
    "    plt.ylabel(metric.capitalize(), fontsize=14)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()\n",
    "\n",
    "print(\"Optimal thresholds for different metrics:\")\n",
    "for metric, threshold in best_thresholds.items():\n",
    "    print(f\"{metric.capitalize()}: {threshold:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Preparing for Kaggle Submission\n",
    "\n",
    "Let's prepare a submission file for the Kaggle competition using our best model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the test data\n",
    "test_files = os.listdir(TEST_DIR)\n",
    "test_ids = [file.split('.')[0] for file in test_files if file.endswith('.tif')]\n",
    "print(f\"Number of test files: {len(test_ids)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a test data generator\n",
    "test_df = pd.DataFrame({'id': test_ids})\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "test_generator = test_datagen.flow_from_dataframe(\n",
    "    dataframe=test_df,\n",
    "    directory=TEST_DIR,\n",
    "    x_col='id',\n",
    "    y_col=None,  # No labels for test data\n",
    "    target_size=(96, 96),\n",
    "    batch_size=32,\n",
    "    class_mode=None,\n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get predictions on the test data using the best model\n",
    "best_model = models[best_model_name]\n",
    "test_predictions = best_model.predict(test_generator, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a submission file\n",
    "submission_df = pd.DataFrame({\n",
    "    'id': test_ids,\n",
    "    'label': (test_predictions > 0.5).astype(int).flatten()\n",
    "})\n",
    "\n",
    "# Save the submission file\n",
    "submission_path = '../submission.csv'\n",
    "submission_df.to_csv(submission_path, index=False)\n",
    "print(f\"Submission file saved to {submission_path}\")\n",
    "print(f\"Number of samples: {len(submission_df)}\")\n",
    "print(f\"Class distribution in submission:\\n{submission_df['label'].value_counts()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Summary and Conclusions\n",
    "\n",
    "In this notebook, we've evaluated and analyzed the performance of our models for the Histopathologic Cancer Detection task. Here's a summary of our findings:\n",
    "\n",
    "1. **Model Performance**: We compared different model architectures (Custom CNN, ResNet50, EfficientNetB0, MobileNetV2) and found that [best model] achieved the highest AUC of [best AUC].\n",
    "\n",
    "2. **Error Analysis**: We analyzed misclassified examples and found patterns in false positives and false negatives. This analysis revealed that [insights from error analysis].\n",
    "\n",
    "3. **Threshold Optimization**: We optimized the classification threshold for different metrics and found that a threshold of [best threshold] maximizes the F1 score.\n",
    "\n",
    "4. **Kaggle Submission**: We prepared a submission file for the Kaggle competition using our best model.\n",
    "\n",
    "In the next notebook (05_Conclusions), we'll summarize our overall findings, discuss the limitations of our approach, and suggest potential improvements for future work."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
